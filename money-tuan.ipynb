{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport os\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import layers\n# Gọi các thư viện cần thiết \nimport pandas as pd # Xu lý bảng\nimport seaborn as sns # Vẽ biểu đồ thị của dữ liệu\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler # Xử lý chuẩn hóa dữ liệu\nfrom sklearn.model_selection import train_test_split # Chia dữ liệu ra làm 2 phần\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization, LSTM    # LSTM  biên dạng ANN, BatchNormalization: cho nhỏ lại\nfrom keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical # Sử dung để làm nổi đối tượng cần phân loại\nfrom keras import callbacks \nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score # Để đo lường\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom tensorflow.keras.preprocessing import image\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras import callbacks\nimport keras\nfrom keras.layers import Dense # fully connected\nfrom keras.datasets import boston_housing\nfrom tensorflow.keras.optimizers import RMSprop # toi uu\nfrom keras.callbacks import EarlyStopping # dung lai ngay lap tuc\nfrom sklearn.preprocessing import scale # xu li du lieu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-22T14:31:42.343735Z","iopub.execute_input":"2022-05-22T14:31:42.344041Z","iopub.status.idle":"2022-05-22T14:31:48.845013Z","shell.execute_reply.started":"2022-05-22T14:31:42.343992Z","shell.execute_reply":"2022-05-22T14:31:48.844321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load 1 image\nimg = image.load_img(\"../input/vnd-data/money/500000/500k (10).jpg\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:31:48.846905Z","iopub.execute_input":"2022-05-22T14:31:48.847745Z","iopub.status.idle":"2022-05-22T14:31:49.061794Z","shell.execute_reply.started":"2022-05-22T14:31:48.847699Z","shell.execute_reply":"2022-05-22T14:31:49.060885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshape data\nimport glob\nm1000 = glob.glob('../input/vnd-data/money/1000/*.*')\nm10000 = glob.glob('../input/vnd-data/money/10000/*.*')\nm100000 = glob.glob('../input/vnd-data/money/100000/*.*')\nm200 = glob.glob('../input/vnd-data/money/200/*.*')\nm2000 = glob.glob('../input/vnd-data/money/2000/*.*')\nm20000 = glob.glob('../input/vnd-data/money/20000/*.*')\nm200000 = glob.glob('../input/vnd-data/money/200000/*.*')\nm500 = glob.glob('../input/vnd-data/money/500/*.*')\nm5000 = glob.glob('../input/vnd-data/money/5000/*.*')\nm50000 = glob.glob('../input/vnd-data/money/50000/*.*')\nm500000 = glob.glob('../input/vnd-data/money/500000/*.*')\n\n\ndata = []\nlabels = []\n\nfor i in m200:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(0)\nfor i in m500:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(1)\nfor i in m1000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(2)\nfor i in m2000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(3)\n\nfor i in m5000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(4)\nfor i in m10000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(5)\nfor i in m20000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(6)\nfor i in m50000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(7)\n        \nfor i in m100000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(8)\nfor i in m200000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(9)\nfor i in m500000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(10)\n\ndata = np.array(data)\nlabels = np.array(labels)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2,\n                                                random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:31:49.063489Z","iopub.execute_input":"2022-05-22T14:31:49.06387Z","iopub.status.idle":"2022-05-22T14:31:52.577406Z","shell.execute_reply.started":"2022-05-22T14:31:49.06384Z","shell.execute_reply":"2022-05-22T14:31:52.576357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape,Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:31:52.579401Z","iopub.execute_input":"2022-05-22T14:31:52.579654Z","iopub.status.idle":"2022-05-22T14:31:52.585647Z","shell.execute_reply.started":"2022-05-22T14:31:52.579623Z","shell.execute_reply":"2022-05-22T14:31:52.584682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape((X_train.shape[0],150,150,3)).astype('float32')/255\nX_test = X_test.reshape((X_test.shape[0],150,150,3)).astype('float32')/255\n\nY_train = to_categorical(Y_train,11)\nY_test = to_categorical(Y_test,11)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:31:52.586818Z","iopub.execute_input":"2022-05-22T14:31:52.587594Z","iopub.status.idle":"2022-05-22T14:31:52.625415Z","shell.execute_reply.started":"2022-05-22T14:31:52.587549Z","shell.execute_reply":"2022-05-22T14:31:52.624648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create model\nfrom keras.layers import Conv2D, MaxPooling2D\nmodel = Sequential()\nmodel.add(Conv2D(32,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same',input_shape=(150,150,3)))\nmodel.add(Conv2D(32,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(64,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 64 lan tich chap\nmodel.add(Conv2D(64,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 128 lan tich chap\nmodel.add(Conv2D(128,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\n\n# model.add(Conv2D(256,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 256 lan tich chap\n# model.add(Conv2D(256,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\n# model.add(MaxPooling2D(2,2))\n\n\nfrom keras.layers import Dense, Activation, Flatten\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(11))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:31:52.626474Z","iopub.execute_input":"2022-05-22T14:31:52.627144Z","iopub.status.idle":"2022-05-22T14:31:52.871386Z","shell.execute_reply.started":"2022-05-22T14:31:52.627109Z","shell.execute_reply":"2022-05-22T14:31:52.870519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\nmodel.compile(loss='mse',optimizer=RMSprop(),metrics=['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs =200, batch_size =160,validation_data=(X_test,Y_test) , verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:31:52.872759Z","iopub.execute_input":"2022-05-22T14:31:52.873142Z","iopub.status.idle":"2022-05-22T15:01:57.488939Z","shell.execute_reply.started":"2022-05-22T14:31:52.873101Z","shell.execute_reply":"2022-05-22T15:01:57.488387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model\nfrom tensorflow.keras.models import load_model\nmodel.save('Money.h5')\nmodel_CNN = load_model('Money.h5')\n\n# Check accuracy\nfrom tensorflow.keras.utils import load_img, img_to_array\nimport numpy as np\nfilename = \"../input/vnd-test/vnd_test/200k_test.jpg\"\n\npredict = ['1000','10000','100000','200','2000','20000','200000','500','5000','50000','500000']\npredict = np.array(predict)\n\n\nimg = load_img(filename,target_size=(150,150))\nimg = img_to_array(img)\nimg = img.reshape(1,150,150,3)\nimg = img.astype('float32')\nimg = img/255\n\nresult = np.argmax(model_CNN.predict(img),axis=-1)\npredict[result]","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:20:48.509511Z","iopub.execute_input":"2022-05-22T15:20:48.510055Z","iopub.status.idle":"2022-05-22T15:20:48.972756Z","shell.execute_reply.started":"2022-05-22T15:20:48.510008Z","shell.execute_reply":"2022-05-22T15:20:48.971859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Results\nscore = model.evaluate(X_test,Y_test, verbose=0)\nprint(\"Loss = \", score[0])\nprint(\"accuracy = \", score[1])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:20:53.406888Z","iopub.execute_input":"2022-05-22T15:20:53.407226Z","iopub.status.idle":"2022-05-22T15:20:54.027836Z","shell.execute_reply.started":"2022-05-22T15:20:53.407199Z","shell.execute_reply":"2022-05-22T15:20:54.02695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw plot\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train','Validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:01:58.648876Z","iopub.execute_input":"2022-05-22T15:01:58.649096Z","iopub.status.idle":"2022-05-22T15:01:58.818753Z","shell.execute_reply.started":"2022-05-22T15:01:58.64907Z","shell.execute_reply":"2022-05-22T15:01:58.817871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}