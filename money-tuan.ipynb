{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport os\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import layers\n# Gọi các thư viện cần thiết \nimport pandas as pd # Xu lý bảng\nimport seaborn as sns # Vẽ biểu đồ thị của dữ liệu\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import StandardScaler # Xử lý chuẩn hóa dữ liệu\nfrom sklearn.model_selection import train_test_split # Chia dữ liệu ra làm 2 phần\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization, LSTM    # LSTM  biên dạng ANN, BatchNormalization: cho nhỏ lại\nfrom keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical # Sử dung để làm nổi đối tượng cần phân loại\nfrom keras import callbacks \nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score # Để đo lường\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom tensorflow.keras.preprocessing import image\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras import callbacks\nimport keras\nfrom keras.layers import Dense # fully connected\nfrom keras.datasets import boston_housing\nfrom tensorflow.keras.optimizers import RMSprop # toi uu\nfrom keras.callbacks import EarlyStopping # dung lai ngay lap tuc\nfrom sklearn.preprocessing import scale # xu li du lieu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T06:18:16.471378Z","iopub.execute_input":"2022-05-23T06:18:16.471974Z","iopub.status.idle":"2022-05-23T06:18:24.392359Z","shell.execute_reply.started":"2022-05-23T06:18:16.471865Z","shell.execute_reply":"2022-05-23T06:18:24.391358Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load 1 image\nimg = image.load_img(\"../input/vnd-data/money/500000/500k (10).jpg\")\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:18:24.393658Z","iopub.execute_input":"2022-05-23T06:18:24.393873Z","iopub.status.idle":"2022-05-23T06:18:24.592427Z","shell.execute_reply.started":"2022-05-23T06:18:24.393847Z","shell.execute_reply":"2022-05-23T06:18:24.591586Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Reshape data\nimport glob\nm1000 = glob.glob('../input/vnd-data/money/1000/*.*')\nm10000 = glob.glob('../input/vnd-data/money/10000/*.*')\nm100000 = glob.glob('../input/vnd-data/money/100000/*.*')\nm200 = glob.glob('../input/vnd-data/money/200/*.*')\nm2000 = glob.glob('../input/vnd-data/money/2000/*.*')\nm20000 = glob.glob('../input/vnd-data/money/20000/*.*')\nm200000 = glob.glob('../input/vnd-data/money/200000/*.*')\nm500 = glob.glob('../input/vnd-data/money/500/*.*')\nm5000 = glob.glob('../input/vnd-data/money/5000/*.*')\nm50000 = glob.glob('../input/vnd-data/money/50000/*.*')\nm500000 = glob.glob('../input/vnd-data/money/500000/*.*')\n\n\ndata = []\nlabels = []\n\nfor i in m200:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(0)\nfor i in m500:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(1)\nfor i in m1000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(2)\nfor i in m2000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(3)\n\nfor i in m5000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(4)\nfor i in m10000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(5)\nfor i in m20000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(6)\nfor i in m50000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(7)\n        \nfor i in m100000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(8)\nfor i in m200000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(9)\nfor i in m500000:   \n    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', \n    target_size= (150,150))\n    image=np.array(image)\n    data.append(image)\n    labels.append(10)\n\ndata = np.array(data)\nlabels = np.array(labels)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2,\n                                                random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:18:24.594002Z","iopub.execute_input":"2022-05-23T06:18:24.594442Z","iopub.status.idle":"2022-05-23T06:18:27.800155Z","shell.execute_reply.started":"2022-05-23T06:18:24.594399Z","shell.execute_reply":"2022-05-23T06:18:27.799272Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape,Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:18:27.801766Z","iopub.execute_input":"2022-05-23T06:18:27.802092Z","iopub.status.idle":"2022-05-23T06:18:27.807742Z","shell.execute_reply.started":"2022-05-23T06:18:27.802017Z","shell.execute_reply":"2022-05-23T06:18:27.806867Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_train = X_train.reshape((X_train.shape[0],150,150,3)).astype('float32')/255\nX_test = X_test.reshape((X_test.shape[0],150,150,3)).astype('float32')/255\n\nY_train = to_categorical(Y_train,11)\nY_test = to_categorical(Y_test,11)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:18:27.809053Z","iopub.execute_input":"2022-05-23T06:18:27.809389Z","iopub.status.idle":"2022-05-23T06:18:27.848344Z","shell.execute_reply.started":"2022-05-23T06:18:27.809356Z","shell.execute_reply":"2022-05-23T06:18:27.847375Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create model\nfrom keras.layers import Conv2D, MaxPooling2D\nmodel = Sequential()\nmodel.add(Conv2D(32,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same',input_shape=(150,150,3)))\nmodel.add(Conv2D(32,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(64,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 64 lan tich chap\nmodel.add(Conv2D(64,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\n\nmodel.add(Conv2D(128,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 128 lan tich chap\nmodel.add(Conv2D(128,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\nmodel.add(MaxPooling2D(2,2))\n\n# model.add(Conv2D(256,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 256 lan tich chap\n# model.add(Conv2D(256,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))\n# model.add(MaxPooling2D(2,2))\n\n\nfrom keras.layers import Dense, Activation, Flatten\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(11))\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:18:27.849587Z","iopub.execute_input":"2022-05-23T06:18:27.849852Z","iopub.status.idle":"2022-05-23T06:18:28.098033Z","shell.execute_reply.started":"2022-05-23T06:18:27.849820Z","shell.execute_reply":"2022-05-23T06:18:28.097059Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Training\nmodel.compile(loss='mse',optimizer=RMSprop(),metrics=['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs =100, batch_size =160,validation_data=(X_test,Y_test) , verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:50:31.560112Z","iopub.execute_input":"2022-05-23T06:50:31.560388Z","iopub.status.idle":"2022-05-23T07:04:19.001617Z","shell.execute_reply.started":"2022-05-23T06:50:31.560359Z","shell.execute_reply":"2022-05-23T07:04:19.000958Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Save model\nfrom tensorflow.keras.models import load_model\nmodel.save('Money.h5')\nmodel_CNN = load_model('Money.h5')\n\n# Check accuracy\nfrom tensorflow.keras.utils import load_img, img_to_array\nimport numpy as np\nfilename = \"../input/test-money/vnd_test_/20000.jpg\"\n\npredict = ['1000','10000','100000','200','2000','20000','200000','500','5000','50000','500000']\npredict = np.array(predict)\n\n\nimg = load_img(filename,target_size=(150,150))\nimg = img_to_array(img)\nimg = img.reshape(1,150,150,3)\nimg = img.astype('float32')\nimg = img/255\n\nresult = np.argmax(model_CNN.predict(img),axis=-1)\npredict[result]","metadata":{"execution":{"iopub.status.busy":"2022-05-23T07:07:44.984027Z","iopub.execute_input":"2022-05-23T07:07:44.984323Z","iopub.status.idle":"2022-05-23T07:07:45.479682Z","shell.execute_reply.started":"2022-05-23T07:07:44.984292Z","shell.execute_reply":"2022-05-23T07:07:45.478856Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Results\nscore = model.evaluate(X_test,Y_test, verbose=0)\nprint(\"Loss = \", score[0])\nprint(\"accuracy = \", score[1])","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:50:04.304335Z","iopub.execute_input":"2022-05-23T06:50:04.305122Z","iopub.status.idle":"2022-05-23T06:50:04.860082Z","shell.execute_reply.started":"2022-05-23T06:50:04.305070Z","shell.execute_reply":"2022-05-23T06:50:04.859035Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Draw plot\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train','Validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T06:45:58.657249Z","iopub.status.idle":"2022-05-23T06:45:58.657775Z","shell.execute_reply.started":"2022-05-23T06:45:58.657464Z","shell.execute_reply":"2022-05-23T06:45:58.657489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}